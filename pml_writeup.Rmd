# Practical Machine Learning Class Project Writeup

## Summary

The goal of this project is to build a machine learning prediction model using the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways, then use the prediction model to predict 20 different test cases of the manner in which the participants did the exercise.

## Data Processing

### Prepare training and cross validation data
We first read in the raw training data csv file, clean up the data by removing columns of all NAs and columns unrelated to accelerometers such as X|user_name|timestamp|new_window|num_window, then partiton data into training data (70%) and cross validation data (30%).

```{r Prepare training and cross validation data}
require(caret)
require(randomForest)
trainRaw <- read.csv("pml-training.csv",na.strings=c("NA",""))
trainNoNA <- trainRaw[,which(colSums(data.frame(is.na(trainRaw)))==0)]
cols2Remove <- grep("X|user_name|timestamp|new_window|num_window",names(trainNoNA))
trainClean <- trainNoNA[,-cols2Remove]
inTrain <- createDataPartition(y = trainClean$classe, p=0.7,list=FALSE)
trainData <- trainClean[inTrain,]
cvData <- trainClean[-inTrain,]
```

```{r train and cv data is ready, echo=TRUE}
dim(trainData)
dim(cvData)
```
### Prepare testing data
We read in the raw testing data csv file, clean up the data by removing columns of all NAs and columns unrelated to accelerometers such as X|user_name|timestamp|new_window|num_window|problem_id.

```{r Prepare testing data}
testRaw <- read.csv("pml-testing.csv",na.strings=c("NA",""))
testNoNA <- testRaw[,which(colSums(data.frame(is.na(testRaw)))==0)]
cols2RemoveTest <- grep("X|user_name|timestamp|new_window|num_window|problem_id",names(testNoNA))
testData <- testNoNA[,-cols2RemoveTest]
```

```{r testing data is ready, echo=TRUE}
dim(testData)
```

## Predication Model Training, Cross Validation and Out of Sample Error Rate

### Model training

We trained 3 different predication models: rpart, gbm and randomForest. 

```{r model training}
fitRPART <- train(classe ~.,method="rpart", data = trainData)
fitRF <- randomForest(classe ~.,data = trainData)
fitGBM <- train(classe ~.,method="gbm", data = trainData2, verbose=FALSE)
```

### Model error rate estimation

gbm estimate of error rate is (1-0.957) = 4.3%
rpart estimate of error rate is (1-0.516) = 48.4%
randomForest OOB estimate of error rate is 0.62%

```{r Model error rate estimation}
fitRPART
fitRF
fitGBM
```
### Cross Validation and Out of Sample Error Rate

```{r Cross Validation and Out of Sample Error Rate, echo=TRUE}
cvPredictionsRPART <- predict(fitRPART,cvData)
error_rate_oos_RPART <- 1-sum(cvData$classe==cvPredictionsRPART)/length(cvData$classe)
error_rate_oos_RPART

cvPredictionsGBM <- predict(fitGBM,cvData)
error_rate_oos_GBM <- 1-sum(cvData$classe==cvPredictionsGBM)/length(cvData$classe)
error_rate_oos_GBM

cvPredictionsRF <- predict(fitRF,cvData)
error_rate_oos_RF <- 1-sum(cvData$classe==cvPredictionsRF)/length(cvData$classe)
error_rate_oos_RF
```
Obviously randomForest provides the best accuracy hence was chosen to predict the test data set.

## Predict Test Cases

Finally we used the above-trainined randomForest model on the above-cleaned test data to predict the 20 test cases. The results have been submitted and the accuracy rate was 100%.

```{r Predict Test Cases, echo=TRUE}
testPredictions <- predict(fitRF,testData)
```

===========================================================================
